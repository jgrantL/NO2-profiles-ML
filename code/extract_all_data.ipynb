{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extract_all_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0kU6wvFuzGPRN9j8jhnDj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n0p_ae1PVe1y"},"source":["## Extract All Rural & Urban Data\n","Author: Jennifer Grant\n"]},{"cell_type":"markdown","metadata":{"id":"l2Ue-eUvVkhT"},"source":["We need to check whether our model can perform well on both rural and urban data. This is important because the Continental United States is made up of urban and rural domains, both of which have completely different properties in their vertical profiles. We need our model to pick up on these differences when making predictions. \n","\n","We need to train our model on all available rural and urban data for Atlanta. This notebook extracts all data from rural and urban cells who have a 35km radius of surrounding information available. This excludes any cells on the outer edges, plus some. Note that 35km is the radius we are using for our model. "]},{"cell_type":"markdown","metadata":{"id":"hLYL9hUkWEGS"},"source":["####Import Libraries"]},{"cell_type":"code","metadata":{"id":"CF48ceMcVG3I","executionInfo":{"status":"ok","timestamp":1595871988269,"user_tz":420,"elapsed":6326,"user":{"displayName":"Jennifer Grant","photoUrl":"","userId":"06630147418387386633"}},"outputId":"7c7bd798-3b3f-4487-f12a-29f1959b1c8e","colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["# Cannot live without our libraries\n","!pip install netCDF4\n","from netCDF4 import Dataset\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from math import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting netCDF4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/39/3687b2ba762a709cd97e48dfaf3ae36a78ae603ec3d1487f767ad58a7b2e/netCDF4-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n","\u001b[K     |████████████████████████████████| 4.3MB 2.7MB/s \n","\u001b[?25hCollecting cftime\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f4/31cb9b65f462ea960bd334c5466313cb7b8af792f272546b68b7868fccd4/cftime-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n","\u001b[K     |████████████████████████████████| 296kB 28.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from netCDF4) (1.18.5)\n","Installing collected packages: cftime, netCDF4\n","Successfully installed cftime-1.2.1 netCDF4-1.5.4\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"eGVTOJRVWHNK"},"source":["#### Import Data"]},{"cell_type":"code","metadata":{"id":"3N96Y7gHWGRv","executionInfo":{"status":"ok","timestamp":1595872006535,"user_tz":420,"elapsed":24573,"user":{"displayName":"Jennifer Grant","photoUrl":"","userId":"06630147418387386633"}},"outputId":"da20804b-fbe7-45c1-d157-6d477f4ab1e3","colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["#first need to mount our drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6W-D_2xcWVxH"},"source":["As mentioned we need to import both urban and rural data. We first start by importing our raw data for urban Atlanta. "]},{"cell_type":"code","metadata":{"id":"mooC52fzWKjW"},"source":["#import urban data files\n","file_endings = ['05', '07', '08', '09', '11', '12', '13', '14']\n","data_path = 'drive/My Drive/Atlanta/raw_data/met_atlanta_full_20'\n","urban_objects = {}\n","\n","for i in np.arange(len(file_endings)):\n","    urban_objects[i] = Dataset(data_path + file_endings[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FR7Z_uI1WdJX"},"source":["Next we import our raw data for rural Atlanta."]},{"cell_type":"code","metadata":{"id":"4dcPqNVtWlL5"},"source":["#import rural data files\n","data_path = 'drive/My Drive/BroaderAtlanta/raw_data/met_broader_atlanta_20'\n","rural_objects = {}\n","\n","for i in np.arange(len(file_endings)):\n","    rural_objects[i] = Dataset(data_path + file_endings[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbOySoZHbHzL"},"source":["We are fortunate enough to have the $cos(zenithangle)$ in our rural Atlanta dataset, but we are not so fortunate for our urban Atlanta dataset. So lastly We must import the zenith angle data for urban Atlanta. "]},{"cell_type":"code","metadata":{"id":"Wx6Ot8XNbrvi"},"source":["#import zenith angle data for our urban dataset (rural already has it)\n","data_path = 'drive/My Drive/Atlanta/zenith_angle_data/met_atlanta_zen_20'\n","zenith_objects = {}\n","\n","for i in np.arange(len(file_endings)):\n","    zenith_objects[i] = Dataset(data_path + file_endings[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVpG3HLPXe5S"},"source":["#### Grabbing Cell Indices"]},{"cell_type":"markdown","metadata":{"id":"UfI8SxAPXmuJ"},"source":["Now that we have our data we need to extract information for each cell represented in our data over the given years. We start this task by finding which cells from both rural & urban datasets have a radius of 35km surrounding information available. If a cell does not have this radius of available surrounding information, then the dataframe cannot be homogenous in dimensions/features as the others, and so we will not use these cells in our model. Note that we use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) below in order to calculate distances that account for the curvature of the earth. "]},{"cell_type":"code","metadata":{"id":"ukxuXI7oXdg4"},"source":["## We first build a couple of functions that will help us find surrounding\n","## indices for each cell.\n","##\n","## Function that returns indices of surrounding cells within a target radius away.\n","def haversine_distance(lons, lats, center_lon, center_lat):\n","  # more variables needed for the formula\n","  a = []\n","  distances = []\n","  earth_radius = 6371.009\n","  lat_diff = [radians(lat - center_lat) for lat in lats]\n","  lon_diff = [radians(lon - center_lon) for lon in lons]\n","  \n","  # need to convert lon,lat & center cell coordinates to radians for equation\n","  x_lat_rad = [radians(lat) for lat in lats]\n","  x_lon_rad = [radians(lon) for lon in lons]\n","  center_lat_rad = radians(center_lat)\n","  center_lon_rad = radians(center_lon)\n","  \n","  #calculate distances using Haversine formula\n","  for i in np.arange(len(lats)):\n","      a.append(sin(lat_diff[i]/2)**2 + cos(x_lat_rad[i])*cos(center_lat_rad)*sin(lon_diff[i]/2)**2)\n","      c = 2*atan2(sqrt(a[i]), sqrt(1 - a[i]))\n","      distances.append(earth_radius*c)\n","  \n","  return find_surrounding_indices(distances)\n","\n","# grabs indices of cells within a target_distance away\n","def find_surrounding_indices(distances):\n","  target_distance = 35\n","  surrounding_indices = []\n","\n","  for distance in distances:\n","    if ((distance <= target_distance) & (distance !=0)): #0 means its our center cell, so we exclude\n","        surrounding_indices.append(distances.index(distance))\n","  return surrounding_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QisELT1FdZyf"},"source":["#grab the lon, lat data from rural datafiles\n","rural_lats = rural_objects[0].variables['xlat'][:][0].data\n","rural_lons = rural_objects[0].variables['xlon'][:][0].data\n","\n","#grab the lon, lat data from urbal datafiles\n","urban_lats = urban_objects[0].variables['xlat'][:][0].data\n","urban_lons = urban_objects[0].variables['xlon'][:][0].data\n","\n","# need number of cells in each dataset - lat/lon tells us this\n","num_rural_cells = len(rural_objects[0].variables['xlat'][0]) \n","num_urban_cells = len(urban_objects[0].variables['xlat'][0])\n","\n","#initialize dictionaries to hold a cell's surrounding indices\n","rural_surroundings = {}\n","urban_surroundings = {}\n","\n","#initialize keys and empty list for each key which will be populated with \n","# surrounding indices\n","for i in np.arange(num_rural_cells):\n","  rural_surroundings[i] = []\n","\n","for i in np.arange(num_urban_cells):\n","  urban_surroundings[i] = []\n","\n","#here we go\n","for i in np.arange(num_rural_cells):\n","  #grab coordinates of cell\n","  center_lat = rural_objects[0].variables['xlat'][:][0][i]\n","  center_lon = rural_objects[0].variables['xlon'][:][0][i]\n","\n","  #call function\n","  rural_surroundings[i].extend(haversine_distance(rural_lons, rural_lats, center_lon, center_lat))\n","\n","for i in np.arange(num_urban_cells):\n","  #grab coordinates of cell\n","  center_lat = urban_objects[0].variables['xlat'][:][0][i]\n","  center_lon = urban_objects[0].variables['xlon'][:][0][i]\n","\n","  #call function\n","  urban_surroundings[i].extend(haversine_distance(urban_lons, urban_lats, center_lon, center_lat))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9PI_fCKj4hVD"},"source":["If you explore the surrounding indices of each cell in either the rural or urban dictionaries populated above, you'll see that they all don't have the same number of surrounding indices. We know that 35km of surrounding information should be 24 cells, so we will filter out any cells that do not have have 24 surrounding indices in the dictionaries."]},{"cell_type":"code","metadata":{"id":"drqOaKS_daJu"},"source":["#initialize constant for number of surrounding indices needed\n","num_surroundings = 24\n","\n","#first filter rural dictionary\n","for i in np.arange(num_rural_cells):\n","  if len(rural_surroundings[i]) != num_surroundings:\n","    del rural_surroundings[i]\n","\n","#next is urban dictionary\n","for i in np.arange(num_urban_cells):\n","  if len(urban_surroundings[i]) != num_surroundings:\n","    del urban_surroundings[i]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYK8oGEK7zpB"},"source":["Fantastic! Last thing we need to adjust is addressing the fact that there are some cells that overlap between our rural and urban datasets. We check to see if any overalapping cells survived the filtering process above and are present in both the rural and urban dictionaries. If so, we keep only one copy - which dictionary we keep it in doesn't matter because there is no feature in our model indicating urban or rural. So long as we include it somewhere."]},{"cell_type":"code","metadata":{"id":"wKVrSlw7dahZ","executionInfo":{"status":"ok","timestamp":1595872094435,"user_tz":420,"elapsed":8040,"user":{"displayName":"Jennifer Grant","photoUrl":"","userId":"06630147418387386633"}},"outputId":"b61882b4-f63f-4e94-a92b-079c3699ee1b","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#lets grab the filtered keys in order to get their coordinates and compare\n","rural_keys = list(rural_surroundings.keys())\n","urban_keys = list(urban_surroundings.keys())\n","\n","#lets put (lon, lats) together as a tuple - easier for comparison\n","rural_coordinates = [(rural_lons[key], rural_lats[key]) for key in rural_keys]\n","urban_coordinates = [(urban_lons[key], urban_lats[key]) for key in urban_keys]\n","\n","#lets check to see if there were actually any overalpping cells removed by \n","# checking length of dictionary before and after\n","old_length = len(urban_surroundings)\n","\n","#now lets compare tuples to find our overlapping cells and remove them from the \n","# urban dictionary\n","for coordinate in urban_coordinates:\n","  if (coordinate in rural_coordinates):\n","    key_of_coordinate = urban_keys[urban_coordinates.index(coordinate)]\n","    del urban_surroundings[key_of_coordinate]\n","\n","#grab new length after removing any potential overlapping cells\n","new_length = len(urban_surroundings)\n","\n","print('Old length: {}\\nNew length: {}'.format(old_length, new_length))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Old length: 17\n","New length: 9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qs_JNx-48tUl","executionInfo":{"status":"ok","timestamp":1595872152841,"user_tz":420,"elapsed":388,"user":{"displayName":"Jennifer Grant","photoUrl":"","userId":"06630147418387386633"}},"outputId":"be06126d-cc26-46b4-e91f-c8dd345ab116","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["rural_surroundings.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys([25, 26, 27, 28, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 109, 110, 111, 112, 113, 114])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"jrnYfjVsXXtB"},"source":["Looks like we did have some overlapping cells afterall. We move onto building dataframes for keys (cells) in both dictionaries."]},{"cell_type":"markdown","metadata":{"id":"CTSb1jo2X4Nd"},"source":["#### Building Dataframes"]},{"cell_type":"markdown","metadata":{"id":"eymbPS2V6C3T"},"source":["We move onto building dataframes for the surviving cells. Note that the rural dataset already has the cosine zenith angle feature in it, while the urban dataset does not. For this reason we need to handle building their respective dataframes differently. "]},{"cell_type":"code","metadata":{"id":"HO9vuGdkYM1n"},"source":["# functions that we will use to help build the dictionary of data to feed into a dataframe object\n","\n","# initialize the number of iterations needed to run loops/list comprehension - \n","# this dimension is common to both rural & urban dataset\n","num_arrays = rural_objects[0].variables['no2'].shape[0] #could have picked any variable with first dimension=744 for either rural or urban (same for both)\n","    \n","def extract_rural_data(label, file, cell_index):\n","  print(label)\n","  data = []\n","  if (label == 'W' or label == 'elev'):\n","      #skip last element (top most layer) for these features to make even 29 layers\n","      for i in np.arange(num_arrays):\n","        data.extend(file.variables[label][i][cell_index].data[:29])\n","  elif (label == 'zenith_angle'):\n","    for i in np.arange(num_arrays):\n","      #take arc cos to get the actual zenith angle\n","      data.append(acos(file.variables['COSZEN'][i][cell_index].data) * 180 / pi)\n","  else:\n","    #extract all layers for every other feature\n","    for i in np.arange(num_arrays):\n","      data.extend(file.variables[label][i][cell_index].data)\n","  return data\n","\n","\n","def extract_urban_data(label, file, zenith_file, cell_index):\n","  print(label)\n","  data = []\n","  if (label == 'W' or label == 'elev'):\n","      #skip last element (top most layer) for these features to make even 29 layers\n","      for i in np.arange(num_arrays):\n","        data.extend(file.variables[label][i][cell_index].data[:29])\n","  elif (label == 'zenith_angle'):\n","    for i in np.arange(num_arrays):\n","      #take arc cos to get the actual zenith angle\n","      data.append(acos(zenith_file.variables['COSZEN'][i][cell_index].data) * 180 / pi)\n","  else:\n","    #extract all layers for every other feature\n","    for i in np.arange(num_arrays):\n","      data.extend(file.variables[label][i][cell_index].data)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LOc10fBYNlC"},"source":["#this function builds a rural dataframe for any cell_index given\n","def build_rural_frame(cell_index):\n","\n","  #build list of feature labels\n","  var_labels = []\n","  for variable in rural_objects[0].variables:    \n","      if (variable == 'xlon' or variable == 'xlat' or variable == 'date' or variable == 'hour'):\n","          continue\n","      elif (variable == 'COSZEN'):\n","          var_labels.append('zenith_angle')\n","      else:\n","          var_labels.append(variable)\n","\n","  #build dictionary that will hold the data\n","  data_dict = {label: [] for label in var_labels} #assign labels so that you can just extend the value with the returned array\n","\n","  for i in np.arange(len(rural_objects)):\n","    #we print the file number and which feature the alogorithm is extracting\n","    #data for\n","    print('\\nFile {}\\n'.format(i + 1))\n","    for variable in var_labels:\n","        data_dict[variable].extend(extract_rural_data(variable, rural_objects[i], cell_index))\n","    \n","  # We need to adjust the cumulative flashcounts to the actual flashcounts\n","  num_months = len(rural_objects) #how many months of data we have\n","  num_per_month = 744 #number of flashcounts per month\n","  updated_ic_flash = []\n","  updated_cg_flash = []\n","  ic_flash = data_dict['IC_FLASHCOUNT']\n","  cg_flash = data_dict['CG_FLASHCOUNT']\n","\n","  for i in np.arange(num_months):\n","    #grab each month separately\n","    month_ic_flashes = ic_flash[i*744:i*744+744]\n","    month_cg_flashes = cg_flash[i*744:i*744+744]\n","\n","    #calculate difference\n","    ic_diff = np.diff(month_ic_flashes)\n","    cg_diff = np.diff(month_cg_flashes)\n","    updated_ic_flash.extend(ic_diff)\n","    updated_cg_flash.extend(cg_diff)\n","\n","    #since we don't have continuous data we append one last zero\n","    updated_ic_flash.append(0)\n","    updated_cg_flash.append(0)\n","\n","  #need to repeat each entry of pblh, ic_flashcount, cg_flashcount, date, hour for\n","  #each vertical layer\n","  data_dict['PBLH'] = np.repeat(data_dict['PBLH'], 29)\n","  data_dict['IC_FLASHCOUNT'] = np.repeat(updated_ic_flash, 29)\n","  data_dict['CG_FLASHCOUNT'] = np.repeat(updated_cg_flash, 29)\n","\n","  # recall that E_NO only had 19 layers and we want to make it 29 to match the\n","  # dimensions of the other features. We fix this by adding an additional ten\n","  # zeros to the end of each 19 layers\n","\n","  zeros = np.zeros(10)\n","  num_obs = int(len(data_dict['E_NO']) / 19)   #how many groups of 19 E_NO we have\n","  e_no = []\n","\n","  for i in np.arange(num_obs):\n","    one_profile = data_dict['E_NO'][i*19:i*19+19]\n","    one_profile.extend(zeros)\n","    e_no.extend(one_profile)\n","\n","  #replace old list in dictionary with updated one\n","  data_dict['E_NO'] = e_no\n","\n","  #extract date and hour, then repeat its entries as well\n","  hour = []\n","  date = []\n","\n","  for i in np.arange(len(rural_objects)):\n","    #flattening the lists so its one list containing all elements\n","    date.extend([item for sublist in rural_objects[i].variables['date'][:] \n","                for item in sublist])\n","    hour.extend([item for sublist in rural_objects[i].variables['hour'][:] \n","                for item in sublist])\n","\n","  dates = np.repeat(date, 29)\n","  hours = np.repeat(hour, 29)\n","\n","  #also need to repeat zenith angle for all 29 layers \n","  data_dict['zenith_angle'] = np.repeat(data_dict['zenith_angle'], 29)\n","\n","  #we add the date and hour lists to the dictionary\n","  data_dict['date'] = dates\n","  data_dict['hour'] = hours\n","  \n","  cell = pd.DataFrame(data_dict)\n","  #putting the date and hour first\n","  cell = cell[['date', 'hour', 'no2', 'U', 'V', 'W', 'PBLH', 'E_NO',\n","                            'IC_FLASHCOUNT', 'CG_FLASHCOUNT', 'pres', 'elev',\n","                            'temp', 'zenith_angle']]\n","  \n","  file_label = 'rural_atlanta_' + str(cell_index) + '.csv'\n","  cell.to_csv('drive/My Drive/urban_and_rural/rural/individual_data/' + file_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwBBewh9YWGe"},"source":["#this function builds an urban dataframe for any cell_index given\n","def build_urban_frame(cell_index):\n","\n","  #build list of feature labels\n","  var_labels = []\n","  for variable in urban_objects[0].variables:    \n","      if (variable == 'xlon' or variable == 'xlat' or variable == 'date' or variable == 'hour'):\n","          continue\n","      else:\n","          var_labels.append(variable)\n","  \n","  #don't have zenith angle in urban dataset so need to append it separately\n","  var_labels.append('zenith_angle')\n","\n","  #build dictionary that will hold the data\n","  data_dict = {label: [] for label in var_labels} #assign labels so that you can just extend the value with the returned array\n","\n","  for i in np.arange(len(urban_objects)):\n","    #we print the file number and which feature the alogorithm is extracting\n","    #data for\n","    print('\\nFile {}\\n'.format(i + 1))\n","    for variable in var_labels:\n","        data_dict[variable].extend(extract_urban_data(variable, urban_objects[i], zenith_objects[i], cell_index))\n","    \n","  # We need to adjust the cumulative flashcounts to the actual flashcounts\n","  num_months = len(urban_objects) #how many months of data we have\n","  num_per_month = 744 #number of flashcounts per month\n","  updated_ic_flash = []\n","  updated_cg_flash = []\n","  ic_flash = data_dict['IC_FLASHCOUNT']\n","  cg_flash = data_dict['CG_FLASHCOUNT']\n","\n","  for i in np.arange(num_months):\n","    #grab each month separately\n","    month_ic_flashes = ic_flash[i*744:i*744+744]\n","    month_cg_flashes = cg_flash[i*744:i*744+744]\n","\n","    #calculate difference\n","    ic_diff = np.diff(month_ic_flashes)\n","    cg_diff = np.diff(month_cg_flashes)\n","    updated_ic_flash.extend(ic_diff)\n","    updated_cg_flash.extend(cg_diff)\n","\n","    #since we don't have continuous data we append one last zero\n","    updated_ic_flash.append(0)\n","    updated_cg_flash.append(0)\n","\n","  #need to repeat each entry of pblh, ic_flashcount, cg_flashcount, date, hour for\n","  #each vertical layer\n","  data_dict['PBLH'] = np.repeat(data_dict['PBLH'], 29)\n","  data_dict['IC_FLASHCOUNT'] = np.repeat(updated_ic_flash, 29)\n","  data_dict['CG_FLASHCOUNT'] = np.repeat(updated_cg_flash, 29)\n","\n","  # recall that E_NO only had 19 layers and we want to make it 29 to match the\n","  # dimensions of the other features. We fix this by adding an additional ten\n","  # zeros to the end of each 19 layers\n","\n","  zeros = np.zeros(10)\n","  num_obs = int(len(data_dict['E_NO']) / 19)   #how many groups of 19 E_NO we have\n","  e_no = []\n","\n","  for i in np.arange(num_obs):\n","    one_profile = data_dict['E_NO'][i*19:i*19+19]\n","    one_profile.extend(zeros)\n","    e_no.extend(one_profile)\n","\n","  #replace old list in dictionary with updated one\n","  data_dict['E_NO'] = e_no\n","\n","  #extract date and hour, then repeat its entries as well\n","  hour = []\n","  date = []\n","\n","  for i in np.arange(len(urban_objects)):\n","    #flattening the lists so its one list containing all elements\n","    date.extend([item for sublist in urban_objects[i].variables['date'][:] \n","                for item in sublist])\n","    hour.extend([item for sublist in urban_objects[i].variables['hour'][:] \n","                for item in sublist])\n","\n","  dates = np.repeat(date, 29)\n","  hours = np.repeat(hour, 29)\n","\n","  #also need to repeat zenith angle for all 29 layers \n","  data_dict['zenith_angle'] = np.repeat(data_dict['zenith_angle'], 29)\n","\n","  #we add the date and hour lists to the dictionary\n","  data_dict['date'] = dates\n","  data_dict['hour'] = hours\n","  \n","  cell = pd.DataFrame(data_dict)\n","  #putting the date and hour first\n","  cell = cell[['date', 'hour', 'no2', 'U', 'V', 'W', 'PBLH', 'E_NO',\n","                            'IC_FLASHCOUNT', 'CG_FLASHCOUNT', 'pres', 'elev',\n","                            'temp', 'zenith_angle']]\n","  \n","  file_label = 'urban_atlanta_' + str(cell_index) + '.csv'\n","  cell.to_csv('drive/My Drive/urban_and_rural/urban/individual_data/' + file_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"93G7sg9LCWKN"},"source":["#lets now call these functions to build our frames\n","for key in rural_keys:\n","  build_rural_frame(key)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TPWvDxGL0eV","executionInfo":{"status":"ok","timestamp":1594856339014,"user_tz":420,"elapsed":785623,"user":{"displayName":"Jennifer Grant","photoUrl":"","userId":"06630147418387386633"}},"outputId":"d2e75e81-f753-48da-9804-963c903d8007","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["urban_keys = list(urban_surroundings.keys())\n","\n","for key in urban_keys:\n","  build_urban_frame(key) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 1\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 2\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 3\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 4\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 5\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 6\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 7\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n","\n","File 8\n","\n","IC_FLASHCOUNT\n","CG_FLASHCOUNT\n","no2\n","U\n","V\n","W\n","PBLH\n","E_NO\n","pres\n","elev\n","temp\n","zenith_angle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yo4Y4Zl0y0gy"},"source":[""],"execution_count":null,"outputs":[]}]}